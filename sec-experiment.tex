\section{Evaluation On Candidate Selection}

In this section, we discuss how we evaluate Sonda on the context of candidate selection and discuss about the results. Our system Sonda was implemented in Ruby and the queries were implemented as SPARQL queries issues over alive SPARQL endpoints. Sonda is available for download at GitHub as a command line tool $\footnote{https://github.com/samuraraujo/ICDE2013}$, as well as all the results that we obtained. 
 
\subsection{Datasets} 

We evaluated our framework using the datasets and ground truth published by the instance-matching benchmark of the Ontology Alignment
Evaluation Initiative (OAEI) \cite{DBLP:journals/jods/EuzenatMSSS11}. We used the datasets provided in 2010 and 2011. We used the life science (LS) collection (which
includes Sider, Drugbank, Dailymed TCM, and Diseasome) and the Person-Restaurant (PR) from the 2010 collection. We excluded LinkedCT from our experiments due to known quality problem in the ground truth. We used all datasets from the 2011 collection. 

\subsection{Evaluation metrics and alternative approaches} 

We used standard metrics, namely Reduction Ratio (RR), Pair-wise Completeness (PP) and F1. Basically, high RR means that the candidate selection algorithm helps to focus on a smaller number of candidates, while high PP means that it could preserve more of the correct candidates. Because the number of all possible candidates is large in this scenario, we use a normalized version of RR. In particular, these metrics are computed as follows: 
\[
\scriptsize\tt
 PC = \frac{\text{ \#Correctly Computed Candidates}} { \text{ \#Ground Truth Candidates}}
\] 
 \[
 \scriptsize\tt
 RR = \frac{\text{\#Instances with Non-Empty Candidate Sets} }{ \text{\#All Computed Candidates} }
\]

\[
\scriptsize\tt
F1 = \frac{2 * RR * PC}{ RR + PC}
\]
Beside these metrics, we also count the average number of queries evaluated per instance as well as overall time for accomplishing the task of leaning the candidate selection scheme and finding(searching) the candidate sets. 

For comparison, we implemented the \emph{S-agnostic}~\cite{papadakis_efficient_2011} and \emph{S-based}~\cite{DBLP:conf/semweb/SongH11} approaches as discussed in Sec.~2. S-based uses only an OR query and it does not feature the branch-and-bound optimization. It requires key pairs, which are generated as in Sec.~3.1. Further, S-based applies a similarity function on the keys to further prune incorrect candidates after that have been retrieved using the OR queries. For comparison purposes, we apply this strategy to all approaches, using the same similarity function. Sonda uses four types of queries for each key pair, and employs the proposed branch-and-bound optimization to select best queries. 
%. We evaluated our approach with all functionalities that we described before (including the predictor , sorting by time, etc.).

\subsection{Querying Candidates} 
We implemented the queries in our algorithm as SPARQL queries (as discussed before) and directly query a SPARQL endpoint to obtain results (limit to 30 instances per query). For that, we loaded all datasets into the OpenLink Virtuoso Universal Server (Version 6.1.5.3127), except for DBPedia, which we queried its on-line sparql endpoint. We use the default S-P-O index created by this server, and created an inverted index for literal values using the following commands:

\lstset{basicstyle=\small}
\begin{lstlisting}[ ]   
DB.DBA.RDF_OBJ_FT_RULE_ADD 
(null, null, 'index_local');
DB.DBA.VT_INC_INDEX_DB_DBA_RDF_OBJ (); 
\end{lstlisting}

We use the specific Virtuoso SPARQL implementation to have access to the index, and we limited all query results to 30 instances. This avoids the queries to retrieve too many data for non-discriminative queries. For example, in this syntax, the 4 query types EXACT, LIKE, AND, and OR are, respectively: 
\lstset{basicstyle=\small}
\begin{lstlisting}[ ]   
 SELECT DISTINCT ?s  WHERE {?s ?p  'eosinophilic pneumonia' .} 
 limit 30
 
 SELECT DISTINCT ?s ?o WHERE {?s ?p ?o .
 ?o bif:contains  '"eosinophilic pneumonia"'  . } limit 30
 
 SELECT DISTINCT ?s ?o WHERE {?s ?p ?o .
 ?o bif:contains  '"eosinophilic"AND"pneumonia"'  . } limit 30
 
 SELECT DISTINCT ?s ?o WHERE {?s ?p ?o .
 ?o bif:contains  '"eosinophilic"OR"pneumonia"'  . } limit 30
\end{lstlisting}


\subsection{Candidate Selection Results} 
Table 1 shows the results. Comparing all approaches over all the 16 datasets pairs that we evaluated, Sonda achieves the best F1 score in all cases (in 96\% of the cases, to be precise), except for the NYT-Geonames pair, where S-based has best F1 result (due to high RR). 
%Sonda yields better F1 score in 96\% of the cases and also
 
\begin{center}
\begin{table*}[h]
\centering
\scriptsize\tt
\caption{Results of the three systems over all pairs of datasets. Queries denote the total number of queries given to the system. Queries/Instance(Q/I) denotes the amount of queries evaluated per instance.} 
    \begin{tabular}{|c|l|c|c|c|c|c|c|c|c|c|}
        \hline
        Dataset Pairs & Systems & Queries & Q/I & Learning(s) & Search(s)  & RR($\%$) & PC($\%$) &  F1($\%$) \\ \hline
 
\multirow{4}{*}{Restaurant1-Restaurant2} & SondaA           \\
											& SondaC  \\
											& S-based \\
 											& S-agnostic     \\ \hline
 											
\multirow{4}{*}{Person11-Person12} & SondaA    \\
											& SondaC  \\
											& S-based \\
 											& S-agnostic      \\ \hline

\multirow{4}{*}{Person21-Person22} & SondaA  \\
											& SondaC  \\
											& S-based \\
 											& S-agnostic       	\\ \hline 										

 \multirow{4}{*}{Sider-Tcm} & SondaA            \\
 											& SondaC  \\
											& S-based \\
 											& S-agnostic        \\ \hline
 											
\multirow{4}{*}{Sider-Dailymed} & SondaA   & 25 & 2.47   & 81.06  & 175.73   & 30.18 & 67.61 & 41.73   \\
											& SondaC  & 75 & 5.43   & 83.09  & 268.66    & 33.94 & 68.89 & 45.48 \\
											& S-based    & 5 & 5.0   & 75.28  & 530.33 & 23.25 & 84.94 & 36.5 \\
 											& S-agnostic     & 3 & 3.0   & 31.73  & 7909.14      & 31.32 & 81.75 & 45.29 \\ \hline 		
 																							
\multirow{4}{*}{Sider-Drugbank} & SondaA  & 25 & 3.26   & 51.19  & 284.51 & 92.68 & 98.49 & 95.49   \\
											& SondaC  \\
											& S-based \\
 											& S-agnostic      \\ \hline 											

\multirow{4}{*}{Sider-Diseasome} & SondaA    & 20 & 2.02   & 10.75  & 24.19     & 52.44 & 95.35 & 67.67\\
											& SondaC    & 20 & 2.03   & 10.2  & 23.11 & 52.44 & 95.35 & 67.67  \\
											& S-based    & 4 & 4.0   & 7.91  & 99.04  & 59.25 & 90.7 & 71.67\\
 											& S-agnostic    & 3 & 3.0   & 1.35  & 49.44  & 61.33 & 88.95 & 72.6      \\ \hline 		 									

\multirow{4}{*}{Dailymed-Sider} & SondaA    & 40 & 1.42   & 101.7  & 263.1   & 98.39 & 99.37 & 98.88  \\
											& SondaC  & 40 & 1.34   & 34.35  & 210.91  & 99.87 & 99.87 & 99.87 \\
											& S-based    & 8 & 8.0   & 28.17  & 1385.41  & 96.85 & 97.99 & 97.42\\
 											& S-agnostic    & 4 & 4.0   & 16.19  & 759.43    & 96.85 & 97.99 & 97.42  \\ \hline 		

\multirow{4}{*}{Diseasome-Sider} & SondaA    & 20 & 1.85   & 12.63  & 17.78   & 97.62 & 95.35 & 96.47  \\
											& SondaC   & 20 & 1.85   & 9.1  & 13.66   & 97.62 & 95.35 & 96.47 \\
											& S-based  & 4 & 4.0   & 6.37  & 51.43   & 85.11 & 93.02 & 88.89 \\
 											& S-agnostic    & 2 & 2.0   & 2.06  & 27.34   & 85.11 & 93.02 & 88.89  \\ \hline 		 									

\multirow{4}{*}{Drugbank-Sider} & SondaA    & 40 & 5.88   & 81.49  & 208.78    & 98.61 & 99.29 & 98.95 \\
											& SondaC	   & 80 & 9.92   & 70.43  & 375.57  & 97.92 & 99.29 & 98.6 \\
											& S-based     & 8 & 8.0   & 53.9  & 273.07  & 92.76 & 99.65 & 96.08\\
 											& S-agnostic   & 26 & 26.0   & 24.56  & 281.62   & 92.46 & 99.65 & 95.92 \\ \hline 											 

\multirow{4}{*}{NYT-Geonames} & SondaA   \\
											& SondaC  \\
											& S-based    \\
 											& S-agnostic     \\ \hline 											


\multirow{4}{*}{NYT-DBPedia(Geo)} & SondaA    \\
											& SondaC  \\
											& S-based \\
 											& S-agnostic       \\ \hline 											
 		
\multirow{4}{*}{NYT-DBPedia(Per)} & SondaA   \\
											& SondaC  \\
											& S-based  \\
 											& S-agnostic       \\ \hline 			
 																			
\multirow{4}{*}{NYT-DBPedia(Corp.)} & SondaA   \\
											& SondaC  \\
											& S-based  \\
 											& S-agnostic       \\ \hline 		 		
 		
\multirow{4}{*}{NYT-Freebase(Geo)} & SondaA  & 20 & 3.59   & 39.89  & 819.35     & 82.55 & 94.43 & 88.09 \\
											& SondaC    & 40 & 4.04   & 31.57  & 978.81   & 86.39 & 95.0 & 90.49\\
											& S-based     & 4 & 4.0   & 24.23  & 1308.69   & 58.07 & 78.7 & 66.83\\
 											& S-agnostic    & 5 & 5.0   & 12.84  & 749.24    & 60.26 & 64.38 & 62.25   \\ \hline 											
 

\multirow{4}{*}{NYT-Freebase(Corp.)} & SondaA   & 15 & 3.02   & 30.51  & 911.26  & 78.06 & 88.27 & 82.85   \\
											 & SondaC   & 15 & 3.09   & 22.09  & 986.65  & 73.21 & 88.17 & 80.0\\
											& S-based     & 3 & 3.0   & 16.73  & 1468.65   & 62.47 & 69.05 & 65.6\\
 											& S-agnostic   & 2 & 2.0   & 13.23  & 622.9       & 63.05 & 43.04 & 51.15    \\ \hline 					
 											
\multirow{4}{*}{NYT-Freebase(Person)} & SondaA    & 15 & 2.12   & 50.75  & 1117.39   & 88.67 & 96.2 & 92.29\\
											& SondaC  &   45 & 3.16   & 45.22  & 1396.57  & 94.62 & 95.62 & 95.12\\
											& S-based       & 3 & 3.0   & 46.14  & 3536.16 &   71.6 & 73.57 & 72.57 \\
 											& S-agnostic    & 4 & 4.0   & 25.87  & 1321.02    & 86.21 & 28.68 & 43.04   \\ \hline 								

\end{tabular}  
\end{table*} 
\end{center}

Overall Sonda ($Sonda_A$ and $Sonda_C$) achieves a considerable improvement in both RR and PC than the other approaches, in the Freebase, DBPedia and Geonames cases. Mainly, due to the high level of ambiguity on those target datasets, those approaches performed worse because they considered only OR queries (that are too inclusive and penalize RR), or do not use comparable keys as in the cases of S-agnostic (penalizing PC).  

\textbf{Class Queries}. The RR in $Sonda_C$ was considerably higher than in $Sonda_A$ for a few cases (DBPedia, Freebase and Geonames). This occurred due to class level ambiguity (many instances with the same name but from different classes) in those datasets. It indicates that the class clauses indeed make the queries more precise because it select exactly the correct class of target instances. However, a little increase on searching time can be observed in $Sonda_C$, mainly due to a higher number of queries that were considered in this version.

\textbf{Time Performance and F1-Measure}. Regarding the learning time performance, in average it takes less than 10\% of the overall searching time. Although, a more complex learning process (that produces better comparable keys) takes more time, it leads to a better F1. 

In general, the best F1 can only be achieved by using an complex learning process and an efficient execution process. For instance, S-based used the same set of comparable keys than Sonda, but it achieved the worse searching time performance in all cases. Even though Sonda has more queries to evaluate (because it uses different types of queries with different time performances), it is faster than S-based, because its  branch-and-bound algorithm helps to select the ones that require less evaluation time (those that are more efficient than the ones used by the other approaches).  In average, S-agnostic has the best learning and searching time performance, but the worse F1. 

Concluding, the best F1 is associated to learn effective comparable keys, which take more time to compute, and to learn efficient queries execution plan, which can efficiently execute the most effective queries. 
 
\textbf{Query Type Effectiveness}. As S-based and S-agnostic considered more inclusive queries (OR queries) than Sonda, we can conclude that, although necessary, more inclusive queries leads to worse F1, mainly because they decrease the RR, and as it was used a limit on queries, it also decrease the PC. 

\textbf{Predictor and Branching Policy Efficiency}. We can see that in all cases, Sonda achieves a considerable reduction in the number of queries evaluated per instance. In some cases (e.g., Dailymed-Sider, Diseasome-Sider), it performed nearly one query per instance. Notice that we can not compare the predictor on S-based and S-agnostic, because in both cases, the number of queries executed per instance is fixed. The purpose here is to show  that Sonda's predictor and branching policy was very efficient, selecting only a few queries per instance, as well as, very effective, selecting queries that produce near optimal PC, in many cases. 


\begin{center}
\begin{table*}[h]
\centering
\scriptsize\tt
\caption{Sonda F1-measure (between precision and recall) compared to ExampleDriven and other tools that participate on the OAEI 2011 benchmark.} 
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Dataset  &  SondaA  &  SondaC & KnoFuss+GA & AggreementMaker & SERIMI & Zhishi.links & ExampleDriven\\ \hline
DBPedia - Geo. & 0 & 0  & 0.89 & 0.69 & 0.68 & 0.92 & 0 \\ \hline
DBPedia - Corp. & 0& 0 & 0.92 & 0.74 & 0.88 & 0.91 & 0\\ \hline
DBPedia - People & 0 & 0 & 0.97 & 0.88 & 0.94 & 0.97 & 0\\ \hline
Freebase - Geo. & 0.90 & 0.90 & 0.93 & 0.85 & 0.91 & 0.88 & 0\\ \hline
Freebase - Corp. & 0.87 & 0.87 & 0.92 & 0.80 & 0.91 & 0.87 & 0\\ \hline
Freebase - People & 0.96 &  0.96 & 0.95 & 0.96 & 0.92 & 0.93 & 0\\ \hline
Geonames & 0 & 0  & 0.90 & 0.85 & 0.80 & 0.91 & 0\\ \hline
Average & 0 & 0  & 0.93 & 0.85 & 0.89 &  0.92 & 0\\ \hline											 
\end{tabular}  
\end{table*} 
\end{center}

\begin{center}
\begin{table*}[h]
\centering
\scriptsize\tt
\caption{Sonda  F1-measure (between precision and recall) compared ExampleDriven and  other tools that participate on the OAEI 2010 benchmark.} 
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Dataset &  SondaA &  SondaC & SERIMI & ObjectCoref & Rimon & ExampleDriven \\ \hline
Sider-Dailymed & 0.63 & 0.61   & 0.66 & - & 0.62  & 0\\ \hline
Sider-Diseasome & 0.90 & 0.90   & 0.87 & - & 0.45 & 0\\ \hline
Sider-Drugbank & 0.93 & 0.93  & 0.97 & - & 0.50  & 0\\ \hline
Sider-TCM & 0 & 0  & 0.97 & - & 0.79 &  0\\ \hline
Dailymed-Sider & 0.93  & 0.94 & 0.67 & 0.70 & 0.62  & 0\\ \hline
Drugbank-Sider & 0.80  & 0.80 & 0.48 & 0.46 & -  & 0\\ \hline
Diseasome-Sider & 0.95 & 0.95   & 0.87 & 0.74 & -  & 0\\ \hline
Person11-Person12 & 0 & 0  & 1.00 & 0.99 & 1.00  & 0\\ \hline
Person21-Person22 & 0 & 0  & 0.46 & 0.95 & 0.97 & 0\\ \hline
Restaurant1-Restaurant2 & 0 & 0  & 0.77 & 0.81  & 0.88 & 0\\ \hline
 								 									 
\end{tabular}  
\end{table*} 
\end{center}



\section{Evaluation On Instance Matching}
In this section, we discuss how we evaluate Sonda on the context of instances matching.  Mainly, we apply two matcher overs the candidates sets produced by Sonda. Then, we compared the results of the match with state-of-the-art instance matching approaches that participate on the OAEI 2010 and 2011 challenge. Also, the ExampleDriven approach was evaluated over the same datasets. As evaluation metrics, we used standard precision, recall and F1 metrics. 

\subsection{Instance Matching Results} 

In this section, we compared the Sonda F1-measure (between precision and recall) with the other alternatives approaches that were evaluated over the same datasets.
%Notice that although not ideal, Sonda completed the task in a reasonable time, considering that our approach queried directly the datasets SPARQL endpoints. 
%The drawback of this approach is that there is a huge time delay due to access to disc, packing of the data in the SPARQL protocol and transferring it via the network. In the other hand, we can integrate two alive data endpoint without any configuration issues, parameter tuning, data pre-processing and indexing, in a couple of minutes or in a few hours. 

%In summary, Sonda yields better F1 score in 96\% of the cases and also, . It means Sonda is more effective in selecting the candidates compared to other approaches, and the main reason for that is attributed to the use of multiple query types and the smart selection of the queries by the branch-and-bound algorithm.


 

 